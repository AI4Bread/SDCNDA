import time
from collections import Counter
from args import get_args
from load_data import load_dataset
from model import Model_our
from model import LogReg
import statistics
import torch_geometric
import torch
import torch as th
import torch.nn as nn
import numpy as np
import warnings
import random
import pdb
import scipy.sparse as sp
import os

seed = 1024
warnings.filterwarnings('ignore')
np.random.seed(seed)
torch.manual_seed(seed)
torch.cuda.manual_seed(seed)
random.seed(seed)
torch.cuda.manual_seed_all(seed)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False
torch.backends.cudnn.enabled = False

args = get_args()

# check cuda
if args.gpu != -1 and th.cuda.is_available():
    args.device = 'cuda:{}'.format(args.gpu)
else:
    args.device = 'cpu'

if __name__ == '__main__':
    print(args)
    # load hyperparameters
    dataname = 'ncrna_disease'  # args.dataname
    hid_dim = args.hid_dim  # args.hid_dim (ä½ è¿™é‡Œæ‰‹åŠ¨å†™æ­»äº†)
    out_dim = args.hid_dim  # âœ… å»ºè®®ï¼šè‹¥ num_MLP=0ï¼Œåˆ™æŠŠ out_dim è®¾æˆ hid_dimï¼›å¦åˆ™ç¡®ä¿ num_MLP>0
    n_layers = args.n_layers
    temp = args.temp
    epochs = 500  # args.epochs
    lr1, wd1 = args.lr1, args.wd1
    lr2, wd2 = args.lr2, args.wd2
    device = args.device

    # ğŸ”¥ åŠ è½½æ•°æ® - ç»Ÿä¸€çš„æ¥å£
    if dataname == 'ncrna_disease':
        data = load_dataset(dataname, 0.6, 0.2, 0.2, 5)
        print("âœ… Loaded ncRNA-disease dataset with enhanced edge information")

        # æ•°æ®é‡Œæä¾›äº†åŒå±‚/è·¨å±‚è¾¹
        edge_index_same = data.edge_index_same  # [2, E_same] (CPU)
        edge_index_cross = data.edge_index_cross  # [2, E_cross] (CPU)

        # ï¼ˆå¯é€‰ä½†æ¨èï¼‰æŠŠä¸¤ç±»è¾¹éƒ½å˜æˆæ— å‘
        from torch_geometric.utils import to_undirected

        edge_index_same = to_undirected(edge_index_same)
        edge_index_cross = to_undirected(edge_index_cross)

    else:
        data = load_dataset(dataname, 0.6, 0.2, 0.2, 5)
        print(f"âœ… Loaded standard dataset: {dataname}")
        # æ ‡å‡†æ•°æ®æ²¡æœ‰åˆ†åŒ/è·¨å±‚ï¼Œå°±å…ˆéƒ½ç”¨åŸå§‹è¾¹
        edge_index_same = data.edge_index
        edge_index_cross = data.edge_index

    import dgl
    from torch_geometric.utils import to_undirected
    import time
    import torch

    # å¤„ç†åŸå§‹å›¾ç»“æ„ï¼ˆç”¨äº ChebNetII çš„è°±æ»¤æ³¢ï¼‰
    data.edge_index = to_undirected(data.edge_index)

    # ç”¨ DGL æ„å»ºå›¾ï¼ˆä»…ä¸ºäº†ä½ ç°æœ‰çš„ conv è°ƒç”¨é‡Œä½¿ç”¨ graph.edge_indexï¼‰
    g = dgl.graph((data.edge_index[0], data.edge_index[1]))
    # åŠ /å»è‡ªç¯ï¼ˆDGL æ˜¯ out-of-place APIï¼Œè®°å¾—æ¥è¿”å›å€¼ï¼‰
    g = g.remove_self_loop().add_self_loop()

    # è®¾å¤‡ä¸ç‰¹å¾
    feat = data.x.to(device)  # [N, F]
    labels = data.y  # ï¼ˆç›®å‰ä¸ç”¨ï¼Œä¸æ¬ä¹Ÿè¡Œï¼‰
    g = g.to(device)

    # ç»™ graph æŒ‚ä¸€ä¸ª PyG é£æ ¼çš„ edge_indexï¼Œä¾› ChebNetII ä½¿ç”¨
    u, v = g.edges()
    # DGL è¿™é‡Œé€šå¸¸è¿”å›åœ¨åŒä¸€ device ä¸Šçš„å¼ é‡ï¼Œä½†ç¨³å¦¥èµ·è§æ˜¾å¼è½¬å‹
    graph = g
    graph.edge_index = torch.stack([u, v]).to(device=device, dtype=torch.long)

    num_class = (torch.max(labels) + 1).item()
    print("Nodes:", feat.shape[0])
    print("Features:", feat.shape[1])
    print("Classes:", num_class)

    in_dim = feat.shape[1]

    # âœ… é‡è¦ï¼šå¦‚æœ num_MLP == 0ï¼Œè¯·æŠŠ out_dim è®¾ä¸º hid_dimï¼Œé¿å…ç»´åº¦ä¸åŒ¹é…
    if (args.num_MLP == 0) and (out_dim != hid_dim):
        print(f"[Note] num_MLP=0 ä¸” out_dim({out_dim})!=hid_dim({hid_dim})ï¼Œå°† out_dim é‡ç½®ä¸º hid_dim ä»¥åŒ¹é…ç›¸ä¼¼åº¦ç»´åº¦ã€‚")
        out_dim = hid_dim

    # åˆå§‹åŒ–æ¨¡å‹ï¼ˆç›´æ¥æŠŠ edge_index_same/cross ä¼ è¿›å»å°±å¥½ï¼‰
    model = Model_our(
        in_dim, hid_dim, out_dim, n_layers, temp,
        args.use_mlp, args.num_MLP, args.gamma, args.k,
        edge_index_same=edge_index_same,  # å…ˆåœ¨ CPUï¼Œforward é‡Œä¼šè‡ªåŠ¨è½¬ device
        edge_index_cross=edge_index_cross
    ).to(device)

    # âŒ ä¸å†éœ€è¦ï¼šmodel.set_edge_indices(...)

    model.batch_size = args.batch_size
    optimizer = torch.optim.Adam(model.parameters(), lr=lr1, weight_decay=wd1)

    print("=== Training ===")
    start = time.time()
    for epoch in range(epochs):
        model.train()
        optimizer.zero_grad()
        loss = model(graph, feat)  # graph.edge_index ä¼šè¢« ChebNetII ä½¿ç”¨
        loss.backward()
        optimizer.step()

        if epoch % 20 == 0 or epoch == epochs - 1:
            print(f'Epoch={epoch:03d}, loss={loss.item():.4f}')
    end = time.time()
    print(f"Training time: {end - start:.2f}s")


    # GPU æ˜¾å­˜ç»Ÿè®¡ï¼ˆå¯é€‰ï¼‰
    def to_MB(byte):
        return byte / 1024.0 / 1024.0


    if torch.cuda.is_available() and 'cuda' in str(device):
        print(f"Max GPU memory: {to_MB(torch.cuda.max_memory_allocated(device)):.2f} MB")

    print("=== Evaluation ===")
    # å†æ¬¡ç¡®ä¿å›¾æœ‰è‡ªç¯ï¼ˆæŒ‰ä½ æµç¨‹ï¼‰
    graph = graph  # å·²ç»åœ¨ä¸Šé¢å¤„ç†è¿‡ï¼›å¦‚æœè¦ä¸¥æ ¼ä¸€è‡´å¯ï¼šg = g.remove_self_loop().add_self_loop()

    # è·å–èŠ‚ç‚¹åµŒå…¥ï¼ˆZ_fï¼‰
    embeds = model.get_embedding(graph, feat)  # [N, out_dim]
    np.savetxt('../result/dataset1/data1.1/embdding.txt', embeds.cpu().detach().numpy())
    print("Embeddings shape:", embeds.shape)
    results = []
